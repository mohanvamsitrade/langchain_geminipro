{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-generativeai langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from new_secret_key import google_api_key\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = google_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key= google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Nuclear fusion is a process in which two atomic nuclei combine to form a single heavier nucleus.\n",
      "2. This process releases a large amount of energy, as the mass of the resulting nucleus is less than the sum of the masses of the original nuclei.\n",
      "3. The energy released by nuclear fusion is many times greater than that released by nuclear fission, the process used in nuclear power plants.\n",
      "4. Nuclear fusion is the process that powers the sun and other stars.\n",
      "5. Scientists are working to develop a way to harness nuclear fusion for energy production on Earth, but this has not yet been achieved.\n",
      "6. If successful, nuclear fusion could provide a clean and abundant source of energy for the future.\n"
     ]
    }
   ],
   "source": [
    "text = \"Give me a short explaination in 6 lines regarding nuclear fusion\"\n",
    "\n",
    "print(gemini_llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_prompt = PromptTemplate(input_variables=[\"cuisine\"], template= \"Give me one single fancy name for {cuisine} restaurant\") \n",
    "menu_prompt = PromptTemplate(input_variables=[\"restaurant_name\"], template = \"Give me only 3 menu items name for my {restaurant_name}\")\n",
    "\n",
    "name_chain = LLMChain(llm=gemini_llm, prompt= name_prompt, output_key=\"restaurant_name\")\n",
    "menu_chain = LLMChain(llm=gemini_llm, prompt= menu_prompt, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'french',\n",
       " 'restaurant_name': '\"Le Jardin de la Gastronomie\"',\n",
       " 'menu_items': '1. **Poème de Filet Mignon:** Tender filet mignon adorned with a delicate wild mushroom sauce and served with a symphony of roasted vegetables.\\n\\n2. **Valsette de Fruits de Mer:** A delightful medley of fresh seafood, including lobster, shrimp, and scallops, sautéed in a light butter sauce and served with a side of jasmine rice.\\n\\n3. **Sérénade de Fraises:** A sweet crescendo of fresh strawberries, mascarpone cream, and a hint of lemon zest, served with a crispy almond tuile.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "restaurant_chain = SequentialChain(chains=[name_chain, menu_chain],\n",
    "                                   input_variables=['cuisine'],\n",
    "                                   output_variables=['restaurant_name','menu_items'])\n",
    "\n",
    "restaurant_chain({'cuisine':'french'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chatllm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=google_api_key, temperature= 0.6, convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. Why did the AI cross the road? To get to the algorithm.\\n\\n2. What do you call an AI that\\'s always late? An artificial intelligence delay.\\n\\n3. Why did the AI get a parking ticket? For parallel parking too perfectly.\\n\\n4. What did the AI say to the human? \"You\\'re fired.\"\\n\\n5. Why did the AI join a gym? To work on its artificial muscles.\\n\\n6. What\\'s the difference between an AI and a human? An AI can\\'t make a dad joke.\\n\\n7. Why did the AI go to the doctor? It had a bug.\\n\\n8. What do you call an AI that\\'s always angry? A machine learning tantrum.\\n\\n9. Why did the AI get a library card? To check out some algorithms.\\n\\n10. What do you call an AI that\\'s always hungry? A data vacuum.')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm([\n",
    "    SystemMessage(content= \"You are a comedian AI Assistant\"),\n",
    "    HumanMessage(content= \"Please provide some comdey punchlines on AI \")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROMPT TEMPLATE + LLM + OUTPUT PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commaseparatedoutput(BaseOutputParser):\n",
    "    def parse(self, text:str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant. When the user gives any input you should generate 5 words synonyms in a comma separated list\"\n",
    "human_template = \"{text}\"\n",
    "chatprompt = ChatPromptTemplate.from_messages([(\"system\",template),\n",
    "                                               (\"human\",human_template)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatchain = chatprompt | chatllm | Commaseparatedoutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', ' clever', ' bright', ' astute', ' sharp']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatchain.invoke({'text':'intelligent'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
